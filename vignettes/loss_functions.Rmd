---
title: "Custom loss functions for SEM"
author: "Erik-Jan van Kesteren"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Custom loss functions for SEM}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = FALSE,
  comment = "#>",
  cache = TRUE
)
```

```{r setup}
library(tensorsem)
```

With `tensorsem` we can estimate structural equation models using maximum likelihood, but we can also go beyond ML to estimate the parameters. For example, we can create a robust form of SEM by using a multivariate t-distribution instead of a multivariate normal distribution for the observations. We can also perform unweighted least squares, or weighted least squares. Lastly, we can put custom penalties on any parameter in the model. 

The machinery that enables all these custom objective functions is automatic differentiation (through `torch`) and adaptive gradient-based optimization algorithms, such as `optim_adam`. In this vignette, we show how to do this.

## Maximum likelihood estimation

Maximum likelihood estimation is the default in `lavaan`, so that's what we'll compare to.
```{r lavaan}
# Create model syntax
syntax <- "
  # three-factor model
  visual  =~ x1 + x2 + x3
  textual =~ x4 + x5 + x6
  speed   =~ x7 + x8 + x9
"

# Fit lavaan model
fit_lavaan <- sem(
  model = syntax,
  data = HolzingerSwineford1939,
  std.lv = TRUE,
  information = "observed",
  fixed.x = FALSE
)

pt_lavaan <- partable(fit_lavaan)
```


And, in order to visually compare the different estimation methods, we create a custom plot function:
```{r paramplot}
param_plot <- function(...) {
  ptli <- list(...)
  ptli <- lapply(names(ptli), function(n) {
    out <- ptli[[n]]
    out$method <- n
    return(out)
  })
  Reduce(rbind, ptli) |> 
    ggplot2::ggplot(ggplot2::aes(x = id, y = est, colour = method,
               ymin = est - 1.96*se, ymax = est + 1.96*se)) + 
    ggplot2::geom_pointrange(position = ggplot2::position_dodge(width = .8)) +
    ggplot2::theme_minimal() +
    ggplot2::labs(x = "Parameter", y = "Value", colour = "Method")
}

param_plot(lavaan = pt_lavaan)
```

Now, we will estimate this same model using `tensorsem`:

```{r tensorsem-ml}
# initialize the SEM model object
mod_ml <- torch_sem(syntax, dtype = torch_float64())

# create a data object as a torch tensor
dat_torch <- torch_tensor(
  data = scale(HolzingerSwineford1939[,7:15], scale = FALSE),
  requires_grad = FALSE,
  dtype = torch_float64(),
)

# estimate the model using default settings
mod_ml$fit(dat = dat_torch)

# re-compute the log-likelihood & create partable
ll <- mod_ml$loglik(dat_torch)
pt_ml <- mod_ml$partable(-ll)

# compare to lavaan
param_plot(
  lavaan = pt_lavaan,
  torch_ml = pt_ml
)

```

As we can see, the two methods yield practically the same parameter estimates and standard errors.

# Robust t-distributed SEM

Next, we'll change the loss function to the negative log-likelihood of a multivariate t-distribution.

```{r tdistribution}

```
