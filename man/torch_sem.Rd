% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/torch_sem.R
\name{torch_sem}
\alias{torch_sem}
\title{Structural equation model with a Torch backend}
\usage{
torch_sem(syntax, dtype = torch_float32(), device = torch_device("cpu"))
}
\arguments{
\item{syntax}{lavaan syntax for the SEM model}

\item{dtype}{(optional) torch dtype for the model (default torch_float32())}

\item{device}{(optional) device type to put the model on. see \code{\link[torch:torch_device]{torch::torch_device()}}}
}
\value{
A \code{torch_sem} object, which is an \code{nn_module} (torch object)
}
\description{
Function for creating a structural equation model
}
\details{
This function instantiates a torch object for computing the model-implied covariance matrix
based on a structural equation model. Through \code{torch}, gradients of this forward model can then
be computed using backpropagation, and the parameters can be optimized using gradient-based
optimization routines from the \code{torch} package.

Because of this, it is easy to add additional penalties to the standard objective function,
or to write a new objective function altogether.
}
\section{Fields}{

\describe{
\item{\code{dlt}}{delta vector: all the parameters of the model (\code{torch_tensor}). Available after the first forward pass.}

\item{\code{free_params}}{Vector of free parameters (R vector)}

\item{\code{Lam}}{Lambda matrix (\code{torch_tensor}). Available after the first forward pass.}

\item{\code{Tht}}{Theta matrix (\code{torch_tensor}). Available after the first forward pass.}

\item{\code{Psi}}{Psi matrix (\code{torch_tensor}). Available after the first forward pass.}

\item{\code{B_0}}{B_0 matrix (\code{torch_tensor}). Available after the first forward pass.}

\item{\code{B}}{B matrix (I - B_0) (\code{torch_tensor}). Available after the first forward pass.}

\item{\code{B_inv}}{Inverse of B matrix (\code{torch_tensor}). Available after the first forward pass.}

\item{\code{Sigma}}{Last computed Sigma matrix (\code{torch_tensor}). Available after the first forward pass.}
}}

\section{Methods}{

\subsection{Forward pass}{

\if{html}{\out{<div class="sourceCode">}}\preformatted{$forward()
}\if{html}{\out{</div>}}

Compute the model-implied covariance matrix.
Don't use this; \code{nn_modules} are callable, so access this method by calling
the object itself as a function, e.g., \code{my_torch_sem()}.
In the forward pass, we apply constraints to the parameter vector, and we
create matrix views from it to compute the model-implied covariance matrix.
\subsection{Value}{

A \code{torch_tensor} of the model-implied covariance matrix
}

}


\subsection{Inverse Hessian}{

\if{html}{\out{<div class="sourceCode">}}\preformatted{$inverse_Hessian(loss)
}\if{html}{\out{</div>}}

Compute and return the asymptotic covariance matrix of the parameters with
respect to the loss function
\subsection{Arguments}{
\itemize{
\item \code{loss} torch_tensor of freshly computed loss function (needed by torch
for backwards pass)
}
}

\subsection{Value}{

A \code{torch_tensor}, representing the ACOV of the free parameters
}

}


\subsection{Standard errors}{

\if{html}{\out{<div class="sourceCode">}}\preformatted{$standard_errors(loss)
}\if{html}{\out{</div>}}

Compute and return observed information standard errors of the
parameters, assuming the loss function is the likelihood and the
current estimates are ML estimates.
\subsection{Arguments}{
\itemize{
\item \code{loss} torch_tensor of freshly computed loss function (needed by torch
for backwards pass)
}
}

\subsection{Value}{

A \verb{numeric vector} of standard errors of the free parameters
}

}


\subsection{Parameter table}{

\if{html}{\out{<div class="sourceCode">}}\preformatted{$partable(loss)
}\if{html}{\out{</div>}}

Create a lavaan-like parameter table from the current parameter estimates in the
torch_sem object.
\subsection{Arguments}{
\itemize{
\item \code{loss} (optional) torch_tensor of freshly computed loss function (needed by torch
for backwards pass)
}
}

\subsection{Value}{

lavaan partable object
}

}


\subsection{Maximum likelihood fitting}{

\if{html}{\out{<div class="sourceCode">}}\preformatted{$fit(dat, lrate, maxit, verbose, tol)
}\if{html}{\out{</div>}}

Fit a torch_sem model using the default maximum likelihood objective.
This function uses the Adam optimizer to estimate the parameters of a torch_sem
\subsection{Arguments}{
\itemize{
\item \code{dat} dataset (centered!) as a \code{torch_tensor}
\item \code{lrate} learning rate of the Adam optimizer.
\item \code{maxit} maximum number of epochs to train the model
\item \code{verbose} whether to print progress to the console
\item \code{tol} parameter change tolerance for stopping training
}
}

\subsection{Value}{

Self, i.e., the \code{torch_sem} object with updated parameters
}

}


\subsection{Log-likelihood}{

\if{html}{\out{<div class="sourceCode">}}\preformatted{$loglik(dat)
}\if{html}{\out{</div>}}

Multivariate normal log-likelihood of the data.
\subsection{Arguments}{
\itemize{
\item \code{dat} dataset (centered!) as a \code{torch_tensor}
}
}

\subsection{Value}{

Log-likelihood value (torch scalar)
}

}
}

\examples{
# create a lavaan syntax for holzinger-swineford data
syntax <- "
  # three-factor model
  visual  =~ x1 + x2 + x3
  textual =~ x4 + x5 + x6
  speed   =~ x7 + x8 + x9
"

# create SEM torch model
model <- torch_sem(syntax)

# prepare dataset
dat <- df_to_tensor(HolzingerSwineford1939[,7:15])

# fit model
model$fit(dat)

# compute loss
loss <- -model$loglik(dat)

# show parameter table
model$partable(loss)
}
\seealso{
\code{\link[=df_to_tensor]{df_to_tensor()}}
}
